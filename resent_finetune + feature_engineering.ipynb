{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -n -q CollabDiff.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-25T06:24:52.876772Z",
     "iopub.status.busy": "2024-10-25T06:24:52.876350Z",
     "iopub.status.idle": "2024-10-25T06:26:40.817265Z",
     "shell.execute_reply": "2024-10-25T06:26:40.816062Z",
     "shell.execute_reply.started": "2024-10-25T06:24:52.876740Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!gdown 'https://drive.google.com/uc?id=1A0xoL44Yg68ixd-FuIJn2VC4vdZ6M2gn'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PosixPath\n",
    "image_path = PosixPath(\"e4e\")  # Standard quotes\n",
    "train_dir = image_path / \"train\"  # Standard quotes\n",
    "test_dir = image_path / \"val\"  # Standard quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T03:39:33.300276Z",
     "iopub.status.busy": "2024-10-25T03:39:33.299405Z",
     "iopub.status.idle": "2024-10-25T03:39:59.185420Z",
     "shell.execute_reply": "2024-10-25T03:39:59.184646Z",
     "shell.execute_reply.started": "2024-10-25T03:39:33.300234Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 2/2 [00:00<00:00, 71.46it/s]\n",
      "Loading dataset: 100%|██████████| 2/2 [00:00<00:00, 2449.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, limit=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        for label_folder in tqdm(['0_real', '1_fake'], desc=\"Loading dataset\"):\n",
    "            full_path = os.path.join(root_dir, label_folder)\n",
    "            for idx, file_name in enumerate(os.listdir(full_path)):\n",
    "                if limit and idx >= limit:\n",
    "                    break  # Limit the number of files loaded\n",
    "                if file_name.endswith(('.jpg', '.png', '.jpeg')):  # Ensure image files\n",
    "                    self.image_files.append(os.path.join(full_path, file_name))\n",
    "                    if 'real' in label_folder:\n",
    "                        self.labels.append(0)  # Label 0 for real images\n",
    "                    else:\n",
    "                        self.labels.append(1)  # Label 1 for fake images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Ensure image is 3 channels\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Paths to the training and validation directories\n",
    "train_dir = \"e4e/train\"\n",
    "val_dir = \"e4e/val\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CustomDataset(root_dir=train_dir, transform=data_transforms)\n",
    "val_dataset = CustomDataset(root_dir=val_dir, transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: certifi in /scratch/user/nkolloju/venv_name/lib/python3.6/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade certifi\n",
    "import ssl\n",
    "import certifi\n",
    "\n",
    "# Set the SSL context to use certifi's certificates\n",
    "ssl._create_default_https_context = ssl.create_default_context\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T03:46:28.427076Z",
     "iopub.status.busy": "2024-10-25T03:46:28.426278Z",
     "iopub.status.idle": "2024-10-25T04:31:51.031389Z",
     "shell.execute_reply": "2024-10-25T04:31:51.030623Z",
     "shell.execute_reply.started": "2024-10-25T03:46:28.427043Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/job.12028134/ipykernel_198645/3633195889.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/11 [Training]:   0%|          | 0/3646 [00:00<?, ?it/s]/tmp/job.12028134/ipykernel_198645/3633195889.py:58: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Enable mixed precision training\n",
      "Epoch 1/11 [Training]: 100%|██████████| 3646/3646 [00:53<00:00, 68.08it/s]\n",
      "Epoch 1/11 [Validation]: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset is empty or no samples were processed.\n",
      "Epochs without improvement: 1\n",
      "Epoch 1/11, Train Loss: 0.0053, Train Acc: 0.9999, Val Loss: inf, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/11 [Training]:  24%|██▍       | 870/3646 [00:13<00:43, 63.51it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 12.25 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.82 GiB is allocated by PyTorch, and 18.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 147\u001b[0m\n\u001b[1;32m    144\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Train with early stopping: Patience set to 3 (meaning stop if no improvement after 3 epochs)\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m train_loss, val_loss, train_acc, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    149\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Save the final trained model (this may not be the best model if early stopping occurred)\u001b[39;00m\n\u001b[1;32m    152\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_resnet50_finetuned_e4e.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 59\u001b[0m, in \u001b[0;36mtrain_model_with_early_stopping\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, accumulation_steps)\u001b[0m\n\u001b[1;32m     56\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():  \u001b[38;5;66;03m# Enable mixed precision training\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     61\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps  \u001b[38;5;66;03m# Normalize loss for gradient accumulation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:155\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[0;32m--> 155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 12.25 MiB is free. Including non-PyTorch memory, this process has 39.35 GiB memory in use. Of the allocated memory 38.82 GiB is allocated by PyTorch, and 18.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained ResNet50 and fine-tune\n",
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "# Replace the final layer for binary classification (real/fake)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # 2 classes (real/fake)\n",
    "\n",
    "# Hook functions to capture low, mid, and high-level features\n",
    "low_level_features, mid_level_features, high_level_features = [], [], []\n",
    "\n",
    "def hook_fn(module, input, output, storage_list):\n",
    "    storage_list.append(output.clone().detach())\n",
    "\n",
    "# Register forward hooks to capture intermediate features\n",
    "model.layer1[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, low_level_features))  # Low-level features\n",
    "model.layer3[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, mid_level_features))  # Mid-level features\n",
    "model.layer4[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, high_level_features))  # High-level features\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Mixed precision training with GradScaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training with hooks and early stopping\n",
    "def train_model_with_early_stopping(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, patience=3, accumulation_steps=4\n",
    "):\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss\n",
    "    epochs_without_improvement = 0  # Track epochs with no improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        # Training loop with progress bar\n",
    "        train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\")\n",
    "        optimizer.zero_grad()  # Zero gradients at the start\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader_iter):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with autocast():  # Enable mixed precision training\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss / accumulation_steps  # Normalize loss for gradient accumulation\n",
    "\n",
    "            scaler.scale(loss).backward()  # Scale the loss for stability\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()  # Zero gradients after the step\n",
    "\n",
    "            # Update running loss and accuracy\n",
    "            running_loss += loss.item() * inputs.size(0) * accumulation_steps  # Undo normalization\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            del inputs, labels, outputs, loss  # Free memory\n",
    "            torch.cuda.empty_cache()  # Clear unused memory\n",
    "\n",
    "        # Calculate training loss and accuracy\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct.double() / total\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "        val_loader_iter = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\")\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for inputs, labels in val_loader_iter:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                with autocast():  # Mixed precision in validation\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += torch.sum(preds == labels.data)\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "                del inputs, labels, outputs, loss  # Free memory\n",
    "                torch.cuda.empty_cache()  # Clear unused memory\n",
    "\n",
    "        # Calculate validation loss and accuracy\n",
    "        if total_val > 0:\n",
    "            val_loss = val_loss / total_val\n",
    "            val_acc = correct_val.double() / total_val\n",
    "            val_loss_history.append(val_loss)\n",
    "            val_acc_history.append(val_acc)\n",
    "        else:\n",
    "            print(\"Validation dataset is empty or no samples were processed.\")\n",
    "            val_loss = float('inf')  # Assign a high loss for empty validation\n",
    "            val_acc = 0.0  # No accuracy for empty validation\n",
    "            val_loss_history.append(val_loss)\n",
    "            val_acc_history.append(val_acc)\n",
    "\n",
    "        # Early stopping logic: Check if validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_resnet50_e4e.pth')  # Save the best model\n",
    "            epochs_without_improvement = 0  # Reset the counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"Epochs without improvement: {epochs_without_improvement}\")\n",
    "        \n",
    "        # Trigger early stopping if no improvement for 'patience' epochs\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Return history for plotting\n",
    "    return train_loss_history, val_loss_history, train_acc_history, val_acc_history\n",
    "\n",
    "# Fine-tune the model with early stopping\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Train with early stopping: Patience set to 3 (meaning stop if no improvement after 3 epochs)\n",
    "train_loss, val_loss, train_acc, val_acc = train_model_with_early_stopping(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=11, patience=2\n",
    ")\n",
    "\n",
    "# Save the final trained model (this may not be the best model if early stopping occurred)\n",
    "torch.save(model.state_dict(), 'final_resnet50_finetuned_e4e.pth')\n",
    "\n",
    "# Plot loss and accuracy curves\n",
    "# Plot loss and accuracy curves\n",
    "def plot_curves(train_loss, val_loss, train_acc, val_acc):\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot loss curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Val Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Convert accuracy tensors to CPU and NumPy\n",
    "    train_acc = [acc.cpu().item() for acc in train_acc]\n",
    "    val_acc = [acc.cpu().item() for acc in val_acc]\n",
    "\n",
    "    # Plot accuracy curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc, label='Train Acc')\n",
    "    plt.plot(epochs, val_acc, label='Val Acc')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig('training_curves_e4e.png')\n",
    "    plt.show()\n",
    "\n",
    "# Call the plot_curves function with your training and validation metrics\n",
    "plot_curves(train_loss, val_loss, train_acc, val_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:28:18.865167Z",
     "iopub.status.busy": "2024-10-25T06:28:18.864812Z",
     "iopub.status.idle": "2024-10-25T06:40:54.399464Z",
     "shell.execute_reply": "2024-10-25T06:40:54.398508Z",
     "shell.execute_reply.started": "2024-10-25T06:28:18.865138Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/job.12028134/ipykernel_198645/43671950.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the saved model\n",
      "/tmp/job.12028134/ipykernel_198645/43671950.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_vit_weights = torch.load(pretrained_weights_path, map_location=device)\n",
      "Loading dataset: 100%|██████████| 2/2 [00:00<00:00, 564.40it/s]\n",
      "Loading dataset: 100%|██████████| 2/2 [00:00<00:00, 1927.97it/s]\n",
      "Loading dataset: 100%|██████████| 2/2 [00:00<00:00, 726.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Train Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1400/1400 [00:46<00:00, 29.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to features_collabDiff/train_features.csv\n",
      "Processing Validation Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 200/200 [00:06<00:00, 30.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to features_collabDiff/val_features.csv\n",
      "Processing Test Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 400/400 [00:39<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to features_collabDiff/test_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data transformations (ResNet-style transformations)\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom dataset class\n",
    "# Custom dataset class\n",
    "class CustomDatasetNew(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, limit=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        for label_folder in tqdm(['real', 'fake'], desc=\"Loading dataset\"):\n",
    "            full_path = os.path.join(root_dir, label_folder)\n",
    "            for idx, file_name in enumerate(os.listdir(full_path)):\n",
    "                if limit and idx >= limit:\n",
    "                    break  # Limit the number of files loaded\n",
    "                if file_name.endswith(('.jpg', '.png', '.jpeg')):  # Ensure image files\n",
    "                    self.image_files.append(os.path.join(full_path, file_name))\n",
    "                    if 'real' in label_folder:\n",
    "                        self.labels.append(0)  # Label 0 for real images\n",
    "                    else:\n",
    "                        self.labels.append(1)  # Label 1 for fake images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]  # This is a string path\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Ensure image is 3 channels\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_path  # Ensure that the path returned is a string (not a tensor)\n",
    "\n",
    "\n",
    "# Load ResNet model and capture features\n",
    "def load_saved_resnet_model(model_path):\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)  # Binary classification (real/fake)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))  # Load the saved model\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Hook functions to capture low, mid, and high-level features\n",
    "    model.layer1[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, low_level_features))\n",
    "    model.layer3[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, mid_level_features))\n",
    "    model.layer4[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, high_level_features))\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Hook functions to capture ResNet features\n",
    "low_level_features, mid_level_features, high_level_features = [], [], []\n",
    "\n",
    "def hook_fn(module, input, output, storage_list):\n",
    "    storage_list.append(output.clone().detach())\n",
    "\n",
    "# Define linear layers to convert ResNet features to 768 dimensions\n",
    "# Define linear layers to convert ResNet features to 768 dimensions\n",
    "low_to_768 = nn.Linear(256, 768).to(device)   # For low-level features\n",
    "mid_to_768 = nn.Linear(1024, 768).to(device)  # For mid-level features\n",
    "high_to_768 = nn.Linear(2048, 768).to(device) # For high-level features\n",
    "\n",
    "def extract_resnet_features(model, image):\n",
    "    low_level_features.clear()\n",
    "    mid_level_features.clear()\n",
    "    high_level_features.clear()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "        model(image)\n",
    "\n",
    "    # Pool ResNet features and map to 768 dimensions\n",
    "    low_pooled = F.adaptive_avg_pool2d(low_level_features[-1].to(device), (1, 1)).squeeze()\n",
    "    mid_pooled = F.adaptive_avg_pool2d(mid_level_features[-1].to(device), (1, 1)).squeeze()\n",
    "    high_pooled = F.adaptive_avg_pool2d(high_level_features[-1].to(device), (1, 1)).squeeze()\n",
    "\n",
    "    low_768 = low_to_768(low_pooled)   # Shape [1, 768]\n",
    "    mid_768 = mid_to_768(mid_pooled)   # Shape [1, 768]\n",
    "    high_768 = high_to_768(high_pooled) # Shape [1, 768]\n",
    "\n",
    "    return low_768, mid_768, high_768\n",
    "\n",
    "\n",
    "# Function to preprocess the image using ViT's transforms\n",
    "def pipeline_preprocessor():\n",
    "    vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    return vit_weights.transforms()\n",
    "\n",
    "# Function to extract ViT embeddings\n",
    "def get_vit_embedding(vit_model, image_path):\n",
    "    preprocessing = pipeline_preprocessor()  # Preprocessing from ViT\n",
    "    img = Image.open(image_path).convert(\"RGB\")  # Ensure we load image by path (string)\n",
    "    img = preprocessing(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feats = vit_model._process_input(img)\n",
    "        batch_class_token = vit_model.class_token.expand(img.shape[0], -1, -1)\n",
    "        feats = torch.cat([batch_class_token, feats], dim=1)\n",
    "        feats = vit_model.encoder(feats)\n",
    "        vit_hidden = feats[:, 0]  # CLS token\n",
    "    return vit_hidden\n",
    "\n",
    "# Load ViT model\n",
    "def load_vit_model(pretrained_weights_path):\n",
    "    vit_model = torchvision.models.vit_b_16(pretrained=False).to(device)\n",
    "    pretrained_vit_weights = torch.load(pretrained_weights_path, map_location=device)\n",
    "    vit_model.load_state_dict(pretrained_vit_weights, strict=False)\n",
    "    vit_model.eval()  # Set to evaluation mode\n",
    "    return vit_model\n",
    "\n",
    "# Add a sequence dimension (if missing) before applying attention\n",
    "def ensure_correct_shape(tensor):\n",
    "    if len(tensor.shape) == 2:  # If shape is [batch_size, embedding_dim]\n",
    "        tensor = tensor.unsqueeze(1)  # Add a sequence dimension: [batch_size, 1, embedding_dim]\n",
    "    elif len(tensor.shape) == 1:  # If shape is [embedding_dim]\n",
    "        tensor = tensor.unsqueeze(0).unsqueeze(1)  # Add batch and sequence dimensions: [1, 1, embedding_dim]\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Scaled dot product attention function\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    # Ensure Q, K, and V have the correct shapes\n",
    "    Q = ensure_correct_shape(Q)  # Should be [batch_size, 1, embedding_dim]\n",
    "    K = ensure_correct_shape(K)  # Should be [batch_size, 1, embedding_dim]\n",
    "    V = ensure_correct_shape(V)  # Should be [batch_size, 1, embedding_dim]\n",
    "\n",
    "#     print(f\"Q shape after unsqueeze: {Q.shape}, K shape after unsqueeze: {K.shape}, V shape after unsqueeze: {V.shape}\")  # Debugging\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32).to(Q.device))\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    return output\n",
    "\n",
    "# Save features for each dataset (train/val/test)\n",
    "import csv\n",
    "\n",
    "# Save features for each dataset (train/val/test) as CSV\n",
    "def save_features_to_csv(model, vit_model, data_loader, save_path):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    with open(save_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the CSV header\n",
    "        writer.writerow([\"image_name\", \"features\", \"label\"])\n",
    "\n",
    "        for images, img_paths in tqdm(data_loader, desc=\"Extracting features\"):\n",
    "            for i in range(len(images)):\n",
    "                image = images[i].to(device)  # Move image to the correct device\n",
    "                img_path = img_paths[i]  # Image path\n",
    "\n",
    "                # Ensure img_path is a string\n",
    "                if isinstance(img_path, torch.Tensor):\n",
    "                    img_path = img_path.item() if img_path.dim() == 0 else str(img_path)\n",
    "\n",
    "                # Extract ResNet features\n",
    "                try:\n",
    "                    low_768, mid_768, high_768 = extract_resnet_features(model, image)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting ResNet features for {img_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Extract ViT features\n",
    "                try:\n",
    "                    vit_hidden = get_vit_embedding(vit_model, img_path)  # img_path should be a string\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting ViT features for {img_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Apply attention between ResNet and ViT features\n",
    "                try:\n",
    "                    output_1 = scaled_dot_product_attention(vit_hidden, low_768, low_768)\n",
    "                    output_2 = scaled_dot_product_attention(output_1, mid_768, mid_768)\n",
    "                    final_output = scaled_dot_product_attention(output_2, high_768, high_768)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error applying attention for {img_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert features to a flattened list\n",
    "                features = final_output.detach().cpu().numpy().flatten().tolist()\n",
    "\n",
    "\n",
    "                # Extract label from the image path\n",
    "                label = 0 if \"real\" in img_path else 1\n",
    "\n",
    "                # Write the row to the CSV\n",
    "                writer.writerow([os.path.basename(img_path), features, label])\n",
    "\n",
    "    print(f\"Features saved to {save_path}\")\n",
    "\n",
    "\n",
    "# Load models\n",
    "resnet_model = load_saved_resnet_model('best_model_resnet50_collabdiff.pth')\n",
    "vit_model = load_vit_model('collabdiff_vit_state_dict.pth')\n",
    "\n",
    "train_dir = \"CollabDiff/train\"\n",
    "val_dir = \"CollabDiff/val\"\n",
    "test_dir=\"CollabDiff/test\"\n",
    "\n",
    "train_dataset = CustomDatasetNew(root_dir=train_dir, transform=data_transforms)\n",
    "val_dataset = CustomDatasetNew(root_dir=val_dir, transform=data_transforms)\n",
    "test_dataset = CustomDatasetNew(root_dir=test_dir, transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"Processing Train Dataset:\")\n",
    "save_features_to_csv(resnet_model, vit_model, train_loader, save_path=\"features_collabDiff/train_features.csv\")\n",
    "\n",
    "print(\"Processing Validation Dataset:\")\n",
    "save_features_to_csv(resnet_model, vit_model, val_loader, save_path=\"features_collabDiff/val_features.csv\")\n",
    "\n",
    "print(\"Processing Test Dataset:\")\n",
    "save_features_to_csv(resnet_model, vit_model, test_loader, save_path=\"features_collabDiff/test_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:42:26.766618Z",
     "iopub.status.busy": "2024-10-25T06:42:26.765657Z",
     "iopub.status.idle": "2024-10-25T06:50:44.644144Z",
     "shell.execute_reply": "2024-10-25T06:50:44.643228Z",
     "shell.execute_reply.started": "2024-10-25T06:42:26.766556Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now handling test directory structure with subfolders (twitter, facebook, reddit)\n",
    "test_dir = \"WildRF/test\"\n",
    "for test_subdir in ['twitter', 'facebook', 'reddit']:\n",
    "    test_dataset = CustomDatasetNew(root_dir=os.path.join(test_dir, test_subdir), transform=data_transforms)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    print(f\"Processing Test Dataset: {test_subdir}\")\n",
    "    save_features(resnet_model, vit_model, test_loader, save_dir=f\"features/test/{test_subdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:59:36.834863Z",
     "iopub.status.busy": "2024-10-25T06:59:36.834496Z",
     "iopub.status.idle": "2024-10-25T06:59:37.869059Z",
     "shell.execute_reply": "2024-10-25T06:59:37.868364Z",
     "shell.execute_reply.started": "2024-10-25T06:59:36.834831Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def folder_to_zip(folder_path, zip_name):\n",
    "    # Check if the folder exists\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(\"The folder does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Create a zip file from the folder\n",
    "    shutil.make_archive(zip_name, 'zip', folder_path)\n",
    "    print(f\"Folder '{folder_path}' has been successfully zipped to '{zip_name}.zip'.\")\n",
    "\n",
    "# Example usage\n",
    "folder_to_zip('/kaggle/working/features', 'features')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 146776,
     "modelInstanceId": 123715,
     "sourceId": 145891,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 146847,
     "modelInstanceId": 123786,
     "sourceId": 145964,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30788,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
